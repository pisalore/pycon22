{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h2 align=center>Automatic Document Generation with Transformers for Layout Analysis Enhancement</h3>\n",
    "<h3 align=center>PyCon 2022</h2>\n",
    "<h3 align=center>Florence, June 3rd 2022</h2>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```python\n",
    "__AUTHOR__ = {'lp': (\"Lorenzo Pisaneschi\",\n",
    "                     \"lorenzo.pisaneschi1@gmail.com\",\n",
    "                     \"https://github.com/pisalore/master_thesis\")}\n",
    "\n",
    "__TOPICS__ = ['Natural Language Processing', 'Text Mining', 'Layout Analysis']\n",
    "\n",
    "__KEYWORDS__ = ['Machine Learning', 'AI', 'NLP', 'Transformers', 'GROBID']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who I am\n",
    "\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <ul>\n",
    "      <li>MSc Degree in Computer Engineering <strong>@University of Study of Florence</strong></li>\n",
    "      <li>Research interests:\n",
    "        <ul>\n",
    "          <li>Natural Language Processing</li>\n",
    "          <li>Machine Learning</li>\n",
    "          <li>Information Extraction</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Backend Developer <strong>@Nephila</strong></li>\n",
    "      <li>First time speaker</li>\n",
    "   </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- Document Layout Analysis\n",
    "\n",
    "- Dataset\n",
    "\n",
    "- Semi-automatic annotation pipeline\n",
    "\n",
    "- Automatic document generation\n",
    "\n",
    "- Method Evaluation and Conclusions\n",
    "\n",
    "- Q&A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Layout Analysis\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/Image0.png\" width=\"380\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li>Identify the most significant parts</li>\n",
    "        <br/>\n",
    "        <li>Do text mining: extract syntactic and semantic information</li>\n",
    "        <br/>\n",
    "        <li>Exploiting state-of-the-art NLP techniques for further inference</li>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Document Layout Analysis\n",
    "### Problems\n",
    "\n",
    "- Needs for a **huge amount of data**\n",
    "\n",
    "- Difficult task: we must deal with many **classes**\n",
    "\n",
    "- High costs: data collection, annotation, pre-processing\n",
    "\n",
    "- Work with **unstructured data**: text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Document Layout Analysis\n",
    "\n",
    "### Ideas\n",
    "\n",
    "- **Use less data**\n",
    "\n",
    "- **Automatically annotate objects**\n",
    "\n",
    "- **Restructure what is unstructured**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed method\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/general.png\" width=\"380\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li style=\"height:150px;\"><strong>Information retrieval, Named Entity Recognition, Text Mining</strong> from unstructured data (PDF)\n",
    "        <li style=\"height:100px;\">Speed up <strong>label processing</strong>\n",
    "        <li style=\"height:50px;\">Generate <strong> synthetic data</strong> exploiting <strong>Transformers</strong>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/image1.png\" width=\"380\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li>Scientific papers in PDF format from ICDAR 2019 (Internation Conference od Document Analysis and Recognition)</li>\n",
    "        <br/>\n",
    "        <li>Specific layout style (IEEE scientific articles)</li>\n",
    "        <br/>\n",
    "        <li>375 PDF for 2088 pages</li>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semi-automatic annotation pipeline\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 480px;\" src=\"images/pipeline.drawio.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GROBID (GeneRation Of BIbliographic Data)\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/input.png\" width=\"450\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li>We need to convert <strong>unstructured</strong> data into parsable <strong>structured</strong> data</li>\n",
    "        <br/>\n",
    "        <li>GROBID makes us available information impossible to retrieve from PDF directly</li>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/pdfs/ICDAR19/1bEnBHAZurlCHlfcg65fed/622U0zHbbvLmKLHzTHo6gZ.pdf...\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "for pdf_path in pathlib.Path(\"data/pdfs\").rglob(\"*.pdf\"):\n",
    "    xml_path = pathlib.Path(\"data/xml\")\n",
    "    xml_path = xml_path.joinpath(\n",
    "        pdf_path.relative_to(\"data/pdfs\").parents[0], pdf_path.stem + \".xml\"\n",
    "    )\n",
    "    xml_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Processing {}...\".format(pdf_path))\n",
    "    # Open PDF\n",
    "    pdf = open(pdf_path, \"rb\")\n",
    "    # Request Grobid xml\n",
    "    xml_response_content = requests.post(\n",
    "        url=\"http://localhost:8070/api/processFulltextDocument\",\n",
    "        files={\"input\": pdf.read()},\n",
    "        data={\n",
    "            \"teiCoordinates\": [\"persName\", \"figure\", \"ref\", \"biblStruct\", \"formula\", \"s\", \"head\",\n",
    "                               ]\n",
    "        },\n",
    "    )\n",
    "    # Write XML file\n",
    "    xml_file = open(xml_path, \"a\")\n",
    "    xml_file.write(xml_response_content.text)\n",
    "    xml_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GROBID examples\n",
    "\n",
    "#### Formula\n",
    "\n",
    "```xml\n",
    "<formula xml:id=\"formula_0\" coords=\"3,119.49,353.98,173.62,25.87\">\n",
    "\tL(x, y) = log e xy n i = 0 e xi\n",
    "\t<label>1</label>\n",
    "</formula>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Table\n",
    "\n",
    "```xml\n",
    "<figure type=\"table\" xml:id=\"tab_2\" coords=\"5,57.42,193.23,496.01,81.97\">\n",
    "\t<head>TABLE III :</head>\n",
    "\t<label>III</label>\n",
    "\t<figDesc>Mean IU (%) on the test set of the different architectures trained on the three manuscripts of the DIVA-HisDB dataset. For pre-training, only the encoder...\n",
    "\t</figDesc>\n",
    "\t<table coords=\"5,65.74,219.88,479.38,55.32\">...\n",
    "\t</table>\n",
    "</figure>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/parsing.png\" width=\"450\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "     Two main tools:\n",
    "     <br>\n",
    "     <ul>\n",
    "        <li><strong>PDFMIner</strong>\n",
    "        <li style=\"height:50px;\"><strong>beautifulSoup4</strong>\n",
    "     </ul>\n",
    "      <li style=\"height:100px;\">The former iterates over <strong>PDF</strong> text and images\n",
    "      <li style=\"height:100px;\">The latter works with relative structured <strong>XML/TEI</strong> file generated by GROBID\n",
    "      <li style=\"height:100px;\">A <code>parser</code> does the work\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What the code does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data/pdfs/ICDAR19/1bEnBHAZurlCHlfcg65fed/622U0zHbbvLmKLHzTHo6gZ.pdf\n",
      "Converting 1bEnBHAZurlCHlfcg65fed from pdf to PNG...\n",
      "PDF file successfully converted.\n"
     ]
    }
   ],
   "source": [
    "% run master_thesis / main --annotations-path data / png --pdfs-path data / pdfs --xml-path data / xml --pickle-filename docs_instances.pickle --debug True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- It aligns information retrieved by **PDF** and **XML** using the ```parse_doc``` function\n",
    "\n",
    "- It save pages as **PNG** for method validation\n",
    "\n",
    "- It saves annotations in `doc_instances.pickle`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parse doc\n",
    "\n",
    "We iterate over **PDF** and related **XML** at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from master_thesis.file_parser.tei import TEIFile\n",
    "from master_thesis.utilities.parser_utils import (\n",
    "    are_similar,\n",
    "    do_overlap,\n",
    "    element_contains_authors,\n",
    "    check_keyword,\n",
    "    calc_coords_from_pdfminer,\n",
    "    check_subtitles,\n",
    "    adjust_overlapping_coordinates,\n",
    ")\n",
    "\n",
    "\n",
    "def parse_doc(pdf_path, xml_path, annotations_path, debug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TEI wrapper class\n",
    "\n",
    "This class is used by `parse_doc` for extract specific content as **properties**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.lxml_soup = self.read_tei(filename)\n",
    "        self.xml_soup = self.read_tei(filename, markup=\"xml\")\n",
    "\n",
    "    def read_tei(self, tei_file, markup=\"lxml\"):\n",
    "        with open(tei_file, \"r\", encoding=\"utf8\") as tei:\n",
    "            return BeautifulSoup(tei, features=\"xml\")\n",
    "\n",
    "    def title(self)\n",
    "\n",
    "    def abstract(self)\n",
    "\n",
    "    def tables(self)\n",
    "\n",
    "    def authors(self)\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Annotations\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/convert_annotations.png\" width=\"450\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "     <br>\n",
    "      <li style=\"height:100px;\">Get and <strong>visualize annotations</strong> for \"debugging\"\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\">Convert them in <strong>PASCAL VOC format</strong>\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\"><strong>Correct</strong> them for better results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### How our annotations look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from master_thesis.utilities.parser_utils import load_doc_instances\n",
    "\n",
    "annotations = load_doc_instances(\"docs_instances.pickle\")\n",
    "paper_name = '622U0zHbbvLmKLHzTHo6gZ'\n",
    "print(annotations[paper_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```python\n",
    "{\n",
    "    \"title\": {\n",
    "        \"content\": \"Selective Super-Resolution for Scene Text Images\\n\",\n",
    "        \"coords\": [162.1726, 92.92924728000003, 448.70301648, 106.59828728000002],\n",
    "    },\n",
    "    \"subtitles\": {\n",
    "        1: [\n",
    "            {\n",
    "                \"title_content\": \"I. INTRODUCTION\",\n",
    "                \"coords\": (139.45, 445.41, 215.64999999999998, 454.61),\n",
    "            },\n",
    "            ...,\n",
    "        ],\n",
    "        2: [\n",
    "            {\n",
    "                \"title_content\": \"III. SUPER-RESOLUTION CONVOLUTIONAL NEURAL NETWORKS\",\n",
    "                \"coords\": (67.29, 220.14, 287.63, 229.33999999999997),\n",
    "            },\n",
    "            ...,\n",
    "        ],\n",
    "    },\n",
    "    # Other object categories\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correction using labelImg\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/labelImg.PNG\" width=\"390\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <br/>\n",
    "    <br/>\n",
    "        <li style=\"height:100px;\">Automatic annotations are a little noisy\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\">Since we want to start with <strong>few data</strong>, we manually correct annotations\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\"><strong>labelImg</strong> is a tool that perfectly suites our needs\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\">Finally we convert PASCAL VOC annotations to <strong>COCO</strong> for LayoutTransformer\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Annotation examples\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/622U0zHbbvLmKLHzTHo6gZ_0.png\" width=\"390\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/622U0zHbbvLmKLHzTHo6gZ_4.png\" width=\"390\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PASCAL VOC annotation example\n",
    "\n",
    "One XML file for **every single page** of each document in initial dataset\n",
    "\n",
    "```XML\n",
    "<annotation>\n",
    "    <folder>1bEnBHAZurlCHlfcg65fed</folder>\n",
    "    <filename>622U0zHbbvLmKLHzTHo6gZ_0.png</filename>\n",
    "    <path>\n",
    "        /home/lorenzo/Documenti/Workshops/pycon2022/data/png/ICDAR19/1bEnBHAZurlCHlfcg65fed/622U0zHbbvLmKLHzTHo6gZ_0.png\n",
    "    </path>\n",
    "    <size>\n",
    "        <width>612</width>\n",
    "        <height>792</height>\n",
    "    </size>\n",
    "    <object>\n",
    "        <name>title</name>\n",
    "        <bndbox>\n",
    "            <xmin>162.1726</xmin>\n",
    "            <xmax>448.70301648</xmax>\n",
    "            <ymin>92.92924728000003</ymin>\n",
    "            <ymax>106.59828728000002</ymax>\n",
    "        </bndbox>\n",
    "    </object>\n",
    "...\n",
    "</annotation>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### COCO annotation example\n",
    "\n",
    "A single file containing all train/validation data to be used with Transformers\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"images\": [{\n",
    "            \"file_name\": \"1qxE3a6GBT9ILRIQA6WzoL/1llA34vNzpDFcSV4Y1Omk2_2.png\",\n",
    "            \"height\": 792,\n",
    "            \"id\": 1001842,\n",
    "            \"width\": 612\n",
    "        },...],\n",
    "    \"annotations\": [{\n",
    "            \"segmentation\": [[68.0, 74.0, 289.0, 74.0, 289.0, 180.0, 68.0, 180.0]],\n",
    "            \"area\": 23426.0,\n",
    "            \"is_crowd\": 0,\n",
    "            \"image_id\": 1001842,\n",
    "            \"bbox\": [68.0, 74.0, 221.0, 106.0],\n",
    "            \"category_id\": 6,\n",
    "            \"id\": 2027900\n",
    "        },...],\n",
    "    \"categories\": [{\n",
    "            \"supercategory\": \"\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"text\"\n",
    "        },...\n",
    "            {\n",
    "            \"supercategory\": \"\",\n",
    "            \"id\": 6,\n",
    "            \"name\": \"figure\"\n",
    "        },\n",
    "        ...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers as generative models\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img width=\"500\" src=\"images/transformres2.png\">\n",
    "    </div>\n",
    "    <div>\n",
    "    <br>\n",
    "        <li><strong>Transformers</strong> revolutionaized the way we do NLP</li>\n",
    "    <br>\n",
    "     <li>Their <strong>encoder-decoder</strong> architecture <strong>differently weights</strong> sequence inpust data significance</li>\n",
    "    <br>\n",
    "        <li>Could we exploit Transformers to understand <strong>instances relationship in layouts</strong>?</li>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LayoutTransformer\n",
    "\n",
    "<div>\n",
    "     <li>Learns <strong>layout schema</strong> and generates novel ones\n",
    "     <li><strong>Scene layout</strong> is a, unordered set of graphical primitive\n",
    "</div>\n",
    "$$\\mathcal{G} = (s_{<bos>}; s_i, x_i, y_i, w_i, h_i, s_{<eos>}, ..., s_n, x_n, y_n, w_n, h_n; s_{<eos>})$$\n",
    "<br>\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 680px;\"\n",
    "  src=\"images/layout_transformer_arch.drawio.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LayoutTransformer training\n",
    "\n",
    "Training is straightforward\n",
    "\n",
    "```shell\n",
    "python main.py\n",
    " --train_json /path/to/annotations/train.json\n",
    " --val_json /path/to/annotations/val.json\n",
    " --exp <exp_name>\n",
    " ```\n",
    "\n",
    "<br>\n",
    "We can also see real-time process progress thanks to **WANDB** tool: https://wandb.ai/site\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training details\n",
    "<div>\n",
    "    <li> <strong>Embeddings input dimension</strong> d = 512\n",
    "    <li> <strong>Layers</strong> = 6\n",
    "    <li> <strong>Heads</strong> = 8\n",
    "    <li> <strong>Learning Rate</strong> = $4.5e-06$\n",
    "    <li> <strong>Adam optimizer</strong>\n",
    "    <li> <strong>Epochs</strong> = 25\n",
    "    <li> <strong>Batch size</strong> = 64\n",
    "    <li> <strong>Teacher forcing</strong>\n",
    "    <li> <strong>Label smoothing</strong>\n",
    "</div>\n",
    "$$ \\textbf{Loss}: E_{\\theta \\sim Disc.}[D_{KL}(SoftMax(\\theta ^L) || p(\\theta ')] + \\lambda E_{\\theta \\sim Cont.}[||\\theta - \\theta '||_1]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference: layouts generation\n",
    "\n",
    "From **LayoutTransformer training** we obtain a **model** which can be used to sample an arbitrary large number of **synthetic new layouts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How we sample synthetic layouts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model_state_path, data_json_path, n_gen_layouts, debug):\n",
    "    dataset = JSONLayout(data_json_path)\n",
    "    model_conf = GPTConfig(\n",
    "        dataset.vocab_size, dataset.max_length, n_layer=6, n_head=8, n_embd=512\n",
    "    )\n",
    "    generative_model = GPT(model_conf)\n",
    "    generative_model.load_state_dict(torch.load(model_state_path))\n",
    "    device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "    generative_model = torch.nn.DataParallel(generative_model).to(device)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        batch_size=len(dataset.data),\n",
    "        num_workers=0,\n",
    "    )\n",
    "    for it in range(n_gen_layouts):\n",
    "        for _, (x, y) in enumerate(loader):\n",
    "            x_cond = x[:1].to(device)\n",
    "            layouts = (\n",
    "                sample(\n",
    "                    generative_model,\n",
    "                    x_cond[:, :6],\n",
    "                    steps=dataset.max_length,\n",
    "                    temperature=1.0,\n",
    "                    sample=False,\n",
    "                    top_k=None,\n",
    "                )\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            for layout in layouts:\n",
    "                layout_dir = Path(exp_dir.joinpath(f\"layout_{it}\"))\n",
    "                layout_dir.mkdir(mode=0o777, parents=False, exist_ok=True)\n",
    "                filename = f\"{PurePosixPath(layout_dir).__str__()}/{it}\"\n",
    "                dataset.save_annotations(layout, f\"{filename}.json\", it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How sample function works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    block_size = (\n",
    "        model.module.get_block_size()\n",
    "        if hasattr(model, \"module\")\n",
    "        else model.getcond_block_size()\n",
    "    )\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = (\n",
    "            x if x.size(1) <= block_size else x[:, -block_size:]\n",
    "        )  # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generated layouts\n",
    "\n",
    "- We obtain a **JSON annotation file** (in COCO format) for **each generated layout**\n",
    "- We synthesized **10k layouts** in **Letter** format (612z792)\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/10.png\" width=\"380\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/20.png\" width=\"360\">\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Postprocessing\n",
    "\n",
    "Since we used **few data**, we obtained **noisy annotations**.\n",
    "\n",
    "We fix them iterating through them to:\n",
    "\n",
    "- **Merge** overlapping objects belonging to same category\n",
    "- **Separate** overlapping objects belonging to different categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example post-processed layouts I\n",
    "\n",
    " <div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/10.png\" width=\"400\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/10_corrected3.png\" width=\"380\">\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example post-processed layouts II\n",
    "\n",
    " <div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/20.png\" width=\"400\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/20_corrected3.png\" width=\"380\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document generation\n",
    "\n",
    "We have layouts, we miss contents.\n",
    "\n",
    "- Generate text from original exploiting **NLTK**\n",
    "- Retrieve images, tables and equation from the web\n",
    "- Integrate formula in synthetic PDF with **LaTeX**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text generation\n",
    "\n",
    "- It is easy with **Natural Language Processing Toolkit** module.\n",
    "- We used the simpler model possible: **3-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "corpus = [word_tokenize(s) for s in all_text]\n",
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, corpus)\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "\n",
    "print(f\"Generate {category} stuff...\")\n",
    "for i in range(text_num):\n",
    "    num_words = all_text_lengths[randrange(0, len(all_text_lengths))]\n",
    "    sentence = generate_sent(model, num_words, random_seed=randint(1, 150))\n",
    "    if sentence and not any(x in sentence for x in [\"(cid:\", \"<s>\", \"</ s>\"]):\n",
    "        generated_instances.append(sentence)\n",
    "# Distinct elements\n",
    "return list(set(generated_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generate sentence\n",
    "\n",
    "- We generate text for all our **text categories** (title, subtitle, text, abstract, keywords, authors, references, caption)\n",
    "- Each model is fitted with **specific text data** taken from original papers during parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words=num_words, random_seed=random_seed):\n",
    "        if token == \"<s>\":\n",
    "            continue\n",
    "        if token == \"</s>\":\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Doclab: put everything together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_synthetic_documents():\n",
    "    for idx, json_path in enumerate(lgt_dir.rglob(\"*.json.json\")):            # Instantiate pdf object (fpdf)\n",
    "        pdf = FPDF(unit=\"pt\")\n",
    "        pdf.add_page()\n",
    "        # Add all necessary fonts for each type of text (titles, subtitles, abstract, authors...) to let them\n",
    "        # available during documents writing\n",
    "        for _, font in FONTS.items():\n",
    "            pdf.add_font(font[\"fontname\"], \"\", font[\"tff\"], uni=True)\n",
    "        filename = json_path.stem.split(\".json\")[0]\n",
    "        # output file path\n",
    "        out_filepath = f\"{gen_pdfs}/{filename}.pdf\"\n",
    "        if not os.path.exists(out_filepath):\n",
    "            generate_document(pdf, json_path, filename, out_filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}