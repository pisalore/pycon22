{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h2 align=center>Automatic Document Generation </br> with Transformers </br> for Layout Analysis Enhancement</h3>\n",
    "<h3 align=center>PyCon 2022</h2>\n",
    "<h3 align=center>Florence, June 3rd 2022</h2>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```python\n",
    "__AUTHOR__ = {'lp': (\"Lorenzo Pisaneschi\",\n",
    "                     \"lorenzo.pisaneschi1@gmail.com\",\n",
    "                     \"https://github.com/pisalore/master_thesis\")}\n",
    "\n",
    "__TOPICS__ = ['Natural Language Processing', 'Text Mining', 'Layout Analysis']\n",
    "\n",
    "__KEYWORDS__ = ['Machine Learning', 'AI', 'NLP', 'Transformers', 'GROBID']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who I am\n",
    "\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <ul>\n",
    "      <li>MSc Degree in Computer Engineering <strong>@University of Study of Florence</strong></li>\n",
    "      <li>Research interests:\n",
    "        <ul>\n",
    "          <li>Natural Language Processing</li>\n",
    "          <li>Machine Learning</li>\n",
    "          <li>Information Extraction</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Backend Developer <strong>@Nephila</strong></li>\n",
    "      <li>First time speaker</li>\n",
    "   </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- Document Layout Analysis\n",
    "\n",
    "- Dataset\n",
    "\n",
    "- Semi-automatic annotation pipeline\n",
    "\n",
    "- Automatic document generation\n",
    "\n",
    "- Method Evaluation and Conclusions\n",
    "\n",
    "- Q&A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Layout Analysis\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/Image0.png\" width=\"380\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li>Identify the most significant parts</li>\n",
    "        <br/>\n",
    "        <li>Do text mining: extract syntactic and semantic information</li>\n",
    "        <br/>\n",
    "        <li>Exploiting state-of-the-art NLP techniques for further inference</li>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Document Layout Analysis\n",
    "### Problems\n",
    "\n",
    "- Needs for a **huge amount of data**\n",
    "\n",
    "- Difficult task: we must deal with many **classes**\n",
    "\n",
    "- **High costs**: data collection, annotation, pre-processing\n",
    "\n",
    "- Work with **unstructured data**: text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Document Layout Analysis\n",
    "\n",
    "### Ideas\n",
    "\n",
    "- **Use less data**\n",
    "\n",
    "- **Automatically annotate objects**\n",
    "\n",
    "- **Restructure what is unstructured**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed method\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/general.png\" width=\"380\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li style=\"height:150px;\"><strong>Information retrieval, Named Entity Recognition, Text Mining</strong> from unstructured data (PDF)\n",
    "        <li style=\"height:100px;\">Speed up <strong>label processing</strong>\n",
    "        <li style=\"height:50px;\">Generate <strong> synthetic data</strong> exploiting <strong>Transformers</strong>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/image1.png\" width=\"380\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li>Scientific papers in PDF format from ICDAR 2019 (Internation Conference od Document Analysis and Recognition)</li>\n",
    "        <br/>\n",
    "        <li>Specific layout style (IEEE scientific articles)</li>\n",
    "        <br/>\n",
    "        <li>375 PDF for 2088 pages</li>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semi-automatic annotation pipeline\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 480px;\" src=\"images/pipeline.drawio.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GROBID (GeneRation Of BIbliographic Data)\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/input.png\" width=\"450\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "        <li>We need to convert <strong>unstructured</strong> data into parsable <strong>structured</strong> data</li>\n",
    "        <br/>\n",
    "        <li>GROBID makes us available information impossible to retrieve from PDF directly</li>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/pdfs/ICDAR19/1bEnBHAZurlCHlfcg65fed/622U0zHbbvLmKLHzTHo6gZ.pdf...\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "for pdf_path in pathlib.Path(\"data/pdfs\").rglob(\"*.pdf\"):\n",
    "    xml_path = pathlib.Path(\"data/xml\")\n",
    "    xml_path = xml_path.joinpath(\n",
    "        pdf_path.relative_to(\"data/pdfs\").parents[0], pdf_path.stem + \".xml\"\n",
    "    )\n",
    "    xml_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Processing {}...\".format(pdf_path))\n",
    "    # Open PDF\n",
    "    pdf = open(pdf_path, \"rb\")\n",
    "    # Request Grobid xml\n",
    "    xml_response_content = requests.post(\n",
    "        url=\"http://localhost:8070/api/processFulltextDocument\",\n",
    "        files={\"input\": pdf.read()},\n",
    "        data={\n",
    "            \"teiCoordinates\": [\"persName\", \"figure\", \"ref\", \"biblStruct\", \"formula\", \"s\", \"head\",\n",
    "                               ]\n",
    "        },\n",
    "    )\n",
    "    # Write XML file\n",
    "    xml_file = open(xml_path, \"a\")\n",
    "    xml_file.write(xml_response_content.text)\n",
    "    xml_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GROBID examples\n",
    "\n",
    "#### Formula\n",
    "\n",
    "```xml\n",
    "<formula xml:id=\"formula_0\" coords=\"3,119.49,353.98,173.62,25.87\">\n",
    "\tL(x, y) = log e xy n i = 0 e xi\n",
    "\t<label>1</label>\n",
    "</formula>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Table\n",
    "\n",
    "```xml\n",
    "<figure type=\"table\" xml:id=\"tab_2\" coords=\"5,57.42,193.23,496.01,81.97\">\n",
    "\t<head>TABLE III :</head>\n",
    "\t<label>III</label>\n",
    "\t<figDesc>Mean IU (%) on the test set of the different architectures trained on the three manuscripts of the DIVA-HisDB dataset. For pre-training, only the encoder...\n",
    "\t</figDesc>\n",
    "\t<table coords=\"5,65.74,219.88,479.38,55.32\">...\n",
    "\t</table>\n",
    "</figure>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/parsing.png\" width=\"450\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "     Two main tools:\n",
    "     <br>\n",
    "     <ul>\n",
    "        <li><strong>PDFMIner</strong>\n",
    "        <li style=\"height:50px;\"><strong>beautifulSoup4</strong>\n",
    "     </ul>\n",
    "      <li style=\"height:100px;\">The former iterates over <strong>PDF</strong> text and images\n",
    "      <li style=\"height:100px;\">The latter works with relative structured <strong>XML/TEI</strong> file generated by GROBID\n",
    "      <li style=\"height:100px;\">A <code>parser</code> does the work\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What the code does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data/pdfs/ICDAR19/1bEnBHAZurlCHlfcg65fed/622U0zHbbvLmKLHzTHo6gZ.pdf\n",
      "Converting 1bEnBHAZurlCHlfcg65fed from pdf to PNG...\n",
      "PDF file successfully converted.\n"
     ]
    }
   ],
   "source": [
    "% run master_thesis/main --annotations-path data/png --pdfs-path data/pdfs --xml-path data/xml --pickle-filename docs_instances.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- It aligns information retrieved by **PDF** and **XML** using the ```parse_doc``` function\n",
    "\n",
    "- It save pages as **PNG** for method validation\n",
    "\n",
    "- It saves annotations in `doc_instances.pickle`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parse doc\n",
    "\n",
    "We iterate over **PDF** and related **XML** at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from master_thesis.file_parser.tei import TEIFile\n",
    "from master_thesis.utilities.parser_utils import (\n",
    "    are_similar,\n",
    "    do_overlap,\n",
    "    element_contains_authors,\n",
    "    check_keyword,\n",
    "    calc_coords_from_pdfminer,\n",
    "    check_subtitles,\n",
    "    adjust_overlapping_coordinates,\n",
    ")\n",
    "\n",
    "\n",
    "def parse_doc(pdf_path, xml_path, annotations_path, debug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TEI wrapper class\n",
    "\n",
    "This class is used by `parse_doc` for extract specific content as **properties**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.lxml_soup = self.read_tei(filename)\n",
    "        self.xml_soup = self.read_tei(filename, markup=\"xml\")\n",
    "\n",
    "    def read_tei(self, tei_file, markup=\"lxml\"):\n",
    "        with open(tei_file, \"r\", encoding=\"utf8\") as tei:\n",
    "            return BeautifulSoup(tei, features=\"xml\")\n",
    "\n",
    "    def title(self)\n",
    "\n",
    "    def abstract(self)\n",
    "\n",
    "    def tables(self)\n",
    "\n",
    "    def authors(self)\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Annotations\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/convert_annotations.png\" width=\"450\">\n",
    "    </div>\n",
    "    <br/>\n",
    "    <br/>\n",
    "     <div>\n",
    "     <br>\n",
    "      <li style=\"height:100px;\">Get and <strong>visualize annotations</strong> for \"debugging\"\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\">Convert them in <strong>PASCAL VOC format</strong>\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\"><strong>Correct</strong> them for better results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### How our annotations look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from master_thesis.utilities.parser_utils import load_doc_instances\n",
    "\n",
    "annotations = load_doc_instances(\"docs_instances.pickle\")\n",
    "paper_name = '622U0zHbbvLmKLHzTHo6gZ'\n",
    "print(annotations[paper_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```python\n",
    "{\n",
    "    \"title\": {\n",
    "        \"content\": \"Selective Super-Resolution for Scene Text Images\\n\",\n",
    "        \"coords\": [162.1726, 92.92924728000003, 448.70301648, 106.59828728000002],\n",
    "    },\n",
    "    \"subtitles\": {\n",
    "        1: [\n",
    "            {\n",
    "                \"title_content\": \"I. INTRODUCTION\",\n",
    "                \"coords\": (139.45, 445.41, 215.64999999999998, 454.61),\n",
    "            },\n",
    "            ...,\n",
    "        ],\n",
    "        2: [\n",
    "            {\n",
    "                \"title_content\": \"III. SUPER-RESOLUTION CONVOLUTIONAL NEURAL NETWORKS\",\n",
    "                \"coords\": (67.29, 220.14, 287.63, 229.33999999999997),\n",
    "            },\n",
    "            ...,\n",
    "        ],\n",
    "    },\n",
    "    # Other object categories\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Annotation examples\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/622U0zHbbvLmKLHzTHo6gZ_0.png\" width=\"390\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/622U0zHbbvLmKLHzTHo6gZ_4.png\" width=\"390\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PASCAL VOC annotation example\n",
    "\n",
    "One XML file for **every single page** of each document in initial dataset\n",
    "\n",
    "```XML\n",
    "<annotation>\n",
    "    <folder>1bEnBHAZurlCHlfcg65fed</folder>\n",
    "    <filename>622U0zHbbvLmKLHzTHo6gZ_0.png</filename>\n",
    "    <path>\n",
    "        /home/lorenzo/pycon2022/data/png/ICDAR19/1bEnBHAZurlCHlfcg65fed/622U0zHbbvLmKLHzTHo6gZ_0.png\n",
    "    </path>\n",
    "    <size>\n",
    "        <width>612</width>\n",
    "        <height>792</height>\n",
    "    </size>\n",
    "    <object>\n",
    "        <name>title</name>\n",
    "        <bndbox>\n",
    "            <xmin>162.1726</xmin>\n",
    "            <xmax>448.70301648</xmax>\n",
    "            <ymin>92.92924728000003</ymin>\n",
    "            <ymax>106.59828728000002</ymax>\n",
    "        </bndbox>\n",
    "    </object>\n",
    "...\n",
    "</annotation>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correction using labelImg\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/labelImg.PNG\" width=\"390\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <br/>\n",
    "    <br/>\n",
    "        <li style=\"height:100px;\">Automatic annotations are a little noisy\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\">Since we want to start with <strong>few data</strong>, we manually correct annotations\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\"><strong>labelImg</strong> is a tool that perfectly suites our needs\n",
    "      <br/>\n",
    "      <li style=\"height:100px;\">Finally we convert PASCAL VOC annotations to <strong>COCO</strong> for LayoutTransformer\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### COCO annotation example\n",
    "\n",
    "A single file containing all train/validation data to be used with Transformers\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"images\": [{\n",
    "            \"file_name\": \"1qxE3a6GBT9ILRIQA6WzoL/1llA34vNzpDFcSV4Y1Omk2_2.png\",\n",
    "            \"height\": 792,\n",
    "            \"id\": 1001842,\n",
    "            \"width\": 612\n",
    "        },...],\n",
    "    \"annotations\": [{\n",
    "            \"segmentation\": [[68.0, 74.0, 289.0, 74.0, 289.0, 180.0, 68.0, 180.0]],\n",
    "            \"area\": 23426.0,\n",
    "            \"is_crowd\": 0,\n",
    "            \"image_id\": 1001842,\n",
    "            \"bbox\": [68.0, 74.0, 221.0, 106.0],\n",
    "            \"category_id\": 6,\n",
    "            \"id\": 2027900\n",
    "        },...],\n",
    "    \"categories\": [{\n",
    "            \"supercategory\": \"\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"text\"\n",
    "        },...\n",
    "            {\n",
    "            \"supercategory\": \"\",\n",
    "            \"id\": 6,\n",
    "            \"name\": \"figure\"\n",
    "        },\n",
    "        ...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers as generative models\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img width=\"500\" src=\"images/transformres2.png\">\n",
    "    </div>\n",
    "    <div>\n",
    "    <br>\n",
    "        <li><strong>Transformers</strong> revolutionaized the way we do NLP</li>\n",
    "    <br>\n",
    "     <li>Their <strong>encoder-decoder</strong> architecture <strong>differently weights</strong> sequence inpust data significance</li>\n",
    "    <br>\n",
    "        <li>Could we exploit Transformers to understand <strong>instances relationship in layouts</strong>?</li>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LayoutTransformer\n",
    "\n",
    "<div>\n",
    "     <li>Learns <strong>layout schema</strong> and generates novel ones\n",
    "     <li><strong>Scene layout</strong> is a, unordered set of graphical primitive\n",
    "</div>\n",
    "$$\\mathcal{G} = (s_{<bos>}; s_i, x_i, y_i, w_i, h_i, s_{<eos>}, ..., s_n, x_n, y_n, w_n, h_n; s_{<eos>})$$\n",
    "<br>\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 680px;\"\n",
    "  src=\"images/layout_transformer_arch.drawio.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LayoutTransformer training\n",
    "\n",
    "Training is straightforward\n",
    "\n",
    "```shell\n",
    "python main.py\n",
    " --train_json /path/to/annotations/train.json\n",
    " --val_json /path/to/annotations/val.json\n",
    " --exp <exp_name>\n",
    " ```\n",
    "\n",
    "<br>\n",
    "We can also see real-time process progress thanks to **WANDB** tool: https://wandb.ai/site\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training details\n",
    "<div>\n",
    "    <li> <strong>Embeddings input dimension</strong> d = 512\n",
    "    <li> <strong>Layers</strong> = 6\n",
    "    <li> <strong>Heads</strong> = 8\n",
    "    <li> <strong>Learning Rate</strong> = $4.5e-06$\n",
    "    <li> <strong>Adam optimizer</strong>\n",
    "    <li> <strong>Epochs</strong> = 25\n",
    "    <li> <strong>Batch size</strong> = 64\n",
    "    <li> <strong>Teacher forcing</strong>\n",
    "    <li> <strong>Label smoothing</strong>\n",
    "</div>\n",
    "$$ \\textbf{Loss}: E_{\\theta \\sim Disc.}[D_{KL}(SoftMax(\\theta ^L) || p(\\theta ')] + \\lambda E_{\\theta \\sim Cont.}[||\\theta - \\theta '||_1]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference: layouts generation\n",
    "\n",
    "From **LayoutTransformer training** we obtain a **model** which can be used to sample an arbitrary large number of **synthetic new layouts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How we sample synthetic layouts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model_state_path, data_json_path, n_gen_layouts, debug):\n",
    "    dataset = JSONLayout(data_json_path)\n",
    "    model_conf = GPTConfig(\n",
    "        dataset.vocab_size, dataset.max_length, n_layer=6, n_head=8, n_embd=512\n",
    "    )\n",
    "    generative_model = GPT(model_conf)\n",
    "    generative_model.load_state_dict(torch.load(model_state_path))\n",
    "    device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "    generative_model = torch.nn.DataParallel(generative_model).to(device)\n",
    "    loader = DataLoader(\n",
    "        dataset, shuffle=True, pin_memory=True, batch_size=len(dataset.data), num_workers=0,\n",
    "    )\n",
    "    for it in range(n_gen_layouts):\n",
    "        for _, (x, y) in enumerate(loader):\n",
    "            x_cond = x[:1].to(device)\n",
    "            layouts = (\n",
    "                sample(\n",
    "                    generative_model, x_cond[:, :6], steps=dataset.max_length, temperature=1.0,\n",
    "                    sample=False, top_k=None,\n",
    "                ).detach().cpu().numpy()\n",
    "            )\n",
    "            for layout in layouts:\n",
    "                layout_dir = Path(exp_dir.joinpath(f\"layout_{it}\"))\n",
    "                layout_dir.mkdir(mode=0o777, parents=False, exist_ok=True)\n",
    "                filename = f\"{PurePosixPath(layout_dir).__str__()}/{it}\"\n",
    "                dataset.save_annotations(layout, f\"{filename}.json\", it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How sample function works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    block_size = (\n",
    "        model.module.get_block_size()\n",
    "        if hasattr(model, \"module\")\n",
    "        else model.getcond_block_size()\n",
    "    )\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = (\n",
    "            x if x.size(1) <= block_size else x[:, -block_size:]\n",
    "        )  # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generated layouts\n",
    "\n",
    "- We obtain a **JSON annotation file** (in COCO format) for **each generated layout**\n",
    "- We synthesized **10k layouts** in **Letter** format (612x792)\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/10.png\" width=\"380\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/20.png\" width=\"360\">\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Postprocessing\n",
    "\n",
    "Since we used **few data**, we obtained **noisy annotations**.\n",
    "\n",
    "We fix them iterating through them to:\n",
    "\n",
    "- **Merge** overlapping objects belonging to same category\n",
    "- **Separate** overlapping objects belonging to different categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example post-processed layouts I\n",
    "\n",
    " <div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/10.png\" width=\"400\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/10_corrected3.png\" width=\"380\">\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example post-processed layouts II\n",
    "\n",
    " <div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/20.png\" width=\"400\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/20_corrected3.png\" width=\"380\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document generation\n",
    "\n",
    "We have layouts, we miss contents.\n",
    "\n",
    "- Generate text from original exploiting **NLTK**\n",
    "- Retrieve images, tables and equation from the web\n",
    "- Integrate formula in synthetic PDF with **LaTeX**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text generation\n",
    "\n",
    "- It is easy with **Natural Language Processing Toolkit** module.\n",
    "- We used the simpler model possible: **3-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "corpus = [word_tokenize(s) for s in all_text]\n",
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, corpus)\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "\n",
    "print(f\"Generate {category} stuff...\")\n",
    "for i in range(text_num):\n",
    "    num_words = all_text_lengths[randrange(0, len(all_text_lengths))]\n",
    "    sentence = generate_sent(model, num_words, random_seed=randint(1, 150))\n",
    "    if sentence and not any(x in sentence for x in [\"(cid:\", \"<s>\", \"</ s>\"]):\n",
    "        generated_instances.append(sentence)\n",
    "# Distinct elements\n",
    "return list(set(generated_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generate sentence\n",
    "\n",
    "- We generate text for all our **text categories** (title, subtitle, text, abstract, keywords, authors, references, caption)\n",
    "- Each model is fitted with **specific text data** taken from original papers during parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words=num_words, random_seed=random_seed):\n",
    "        if token == \"<s>\":\n",
    "            continue\n",
    "        if token == \"</s>\":\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Doclab: put everything together\n",
    "\n",
    "- We iterate through annotations files end annotations themselves to fill the boxes.\n",
    "- Core method is `generate_document`\n",
    "- `FPDF` is the module which instantiates PDF files and allows to modify their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_documents():\n",
    "    for idx, json_path in enumerate(lgt_dir.rglob(\"*.json.json\")):\n",
    "        # Instantiate pdf object (fpdf)\n",
    "        pdf = FPDF(unit=\"pt\")\n",
    "        pdf.add_page()\n",
    "        # Add all necessary fonts for each type of text (titles, subtitles, abstract, authors...)\n",
    "        # to make them available during documents writing\n",
    "        for _, font in FONTS.items():\n",
    "            pdf.add_font(font[\"fontname\"], \"\", font[\"tff\"], uni=True)\n",
    "        filename = json_path.stem.split(\".json\")[0]\n",
    "        # output file path\n",
    "        out_filepath = f\"{gen_pdfs}/{filename}.pdf\"\n",
    "        if not os.path.exists(out_filepath):\n",
    "            generate_document(pdf, json_path, filename, out_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `generate_document`: text\n",
    "- Get the `category` and random text\n",
    "- `cell_kwargs` contains format information for FPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cell_kwargs = {\n",
    "    \"w\": width,\n",
    "    \"h\": font.get(\"h\"),\n",
    "    \"align\": font.get(\"align\"),\n",
    "}\n",
    "texts = gen_text_dict.get(ann_category)\n",
    "text_rows = pdf.multi_cell(\n",
    "    **cell_kwargs,\n",
    "    txt=texts[random.randrange(len(texts))],\n",
    "    split_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `generate_document`: figures and tables\n",
    "\n",
    "- Get the **image** from the web (**VISImageNavigator**)\n",
    "- **Resize** considering annotation bounding box dimension\n",
    "- Add to PDF Instance. **Coordinates** are taken from annotation itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "img = get_image(width, height, visi_img_category)\n",
    "pdf.image(img, xmin, ymin, width, height, \"PNG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `generate_document`: formula\n",
    "\n",
    "- Get the **formula** from the web (equationsheet.com)\n",
    "- **Compile** it via `pylatex`\n",
    "- **Merge** it to PDF instance using **PyPDF2**\n",
    "- Add to PDF Instance. **Coordinates** are taken from annotation itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We need a LaTeX compiler behind the scenes\n",
    "...\n",
    "for i in range(n_eq):\n",
    "    with doc.create(TextBlock(0, x_eq, y_eq + tot_height)) as page:\n",
    "        if tot_height + 40 < f_height:\n",
    "            agn = Alignat(numbering=False, escape=False)\n",
    "            agn.append(random.choice(LATEX_FORMULA))\n",
    "            page.append(agn)\n",
    "            tot_height += 40\n",
    "doc.generate_pdf(filename, clean_tex=True)\n",
    "# Open PDF and merge\n",
    "...\n",
    "merge_pdf_pages(\n",
    "    formula_pdf.getPage(0),\n",
    "    fpdf_pdf.getPage(0),\n",
    "    out_filepath,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generated documents I\n",
    "\n",
    " <div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/generated_pdfs/7_0.png\" width=\"400\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/generated_pdfs/17_0.png\" width=\"380\">\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generated documents II\n",
    "\n",
    " <div>\n",
    "    <div style=\"float: left\">\n",
    "        <img src=\"images/generated_pdfs/20_0.png\" width=\"400\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"images/generated_pdfs/a1096_0.png\" width=\"380\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method evaluation\n",
    "\n",
    "- We generated **10k document pages** using our method\n",
    "\n",
    "- From **2048** pages we instantiated 10k more, fully annotated exploiting **Transformers**\n",
    "\n",
    "- We need to **evaluate** it: does our synthetic dataset help neural networks to **improve** detection performance?\n",
    "\n",
    "\n",
    "|            | Standard Dataset | Augmented Dataset | \\#Added instances |\n",
    "|------------|------------------|-------------------|-------------------|\n",
    "| Text       | 8.567            | 82.990            | 76.423            |\n",
    "| Abstract   | 162              | 1.884             | 1.722             |\n",
    "| Formula    | 694              | 5.864             | 5.170             |\n",
    "| Caption    | 1.312            | 10.370            | 9.058             |\n",
    "| Title      | 162              | 1.884             | 1.722             |\n",
    "| Table      | 528              | 3.291             | 2.763             |\n",
    "| Authors    | 161              | 1.875             | 1.714             |\n",
    "| References | 328              | 1.710             | 1.382             |\n",
    "| Subtitle   | 2.352            | 22.309            | 19.957            |\n",
    "| Figure     | 1.082            | 14.786            | 13.704            |\n",
    "| Keywords   | 130              | 997               | 867               |\n",
    "| Total      | 15.478           | 147.960           | 132.482           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  ResNeXt and Transfer Learning\n",
    "\n",
    "<div>\n",
    "  <ul>\n",
    "      <li>We used <strong>ResNeXt-101-32x8d4 model</strong> pre-trained on <strong>DocBank</strong> dataset</li>\n",
    "      <br>\n",
    "      <li>Three <strong>training sets</strong> to obtain as many <strong>models</strong>:\n",
    "      <br>\n",
    "        <ul>\n",
    "          <li>One formed by 1044 original pages <strong>semi-automatically annotated</strong>\n",
    "          <li>One formed by 1044 original pages + 10.000 synthetic pages <strong>automatically annotated</strong>\n",
    "          <li>One formed by 1044 original pages + 10.000 synthetic pages <strong>semi-automatically annotated</strong>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <br>\n",
    "      <li>One <strong>test set</strong>:\n",
    "      <ul>\n",
    "        <li>Remaining 1044 pages from original dataset <strong>automatically annotated</strong>\n",
    "      </ul>\n",
    "   </ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using detectron2 by Facebook AI\n",
    "\n",
    "It makes available customizable config files. The **base model**:\n",
    "\n",
    "```yaml\n",
    "MODEL:\n",
    "  META_ARCHITECTURE: \"GeneralizedRCNN\"\n",
    "  BACKBONE:\n",
    "    NAME: \"build_resnet_fpn_backbone\"\n",
    "  RESNETS:\n",
    "    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "  FPN:\n",
    "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "  ANCHOR_GENERATOR:\n",
    "    SIZES: [[32], [64], [128], [256], [512]]  # One size for each in feature map\n",
    "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)\n",
    "  RPN:\n",
    "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "    PRE_NMS_TOPK_TRAIN: 2000  # Per FPN level\n",
    "    PRE_NMS_TOPK_TEST: 1000  # Per FPN level\n",
    "    POST_NMS_TOPK_TRAIN: 1000\n",
    "    POST_NMS_TOPK_TEST: 1000\n",
    "  ROI_HEADS:\n",
    "    NAME: \"StandardROIHeads\"\n",
    "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\"]\n",
    "  ROI_BOX_HEAD:\n",
    "    NAME: \"FastRCNNConvFCHead\"\n",
    "    NUM_FC: 2\n",
    "    POOLER_RESOLUTION: 7\n",
    "  ROI_MASK_HEAD:\n",
    "    NAME: \"MaskRCNNConvUpsampleHead\"\n",
    "    NUM_CONV: 4\n",
    "    POOLER_RESOLUTION: 14\n",
    "DATASETS:\n",
    "  TRAIN: (\"train\",)\n",
    "  TEST: (\"val\",)\n",
    "SOLVER:\n",
    "  IMS_PER_BATCH: 16\n",
    "  BASE_LR: 0.02\n",
    "  STEPS: (60000, 80000)\n",
    "  MAX_ITER: 90000\n",
    "INPUT:\n",
    "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
    "VERSION: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **ResNeXt** we used for fine-tuning and experiments:\n",
    "\n",
    "```yaml\n",
    "_BASE_: \"Base-RCNN-FPN.yaml\"\n",
    "MODEL:\n",
    "  WEIGHTS: \"detectron2://ImageNetPretrained/FAIR/X-101-32x8d.pkl\"\n",
    "  MASK_ON: False\n",
    "  PIXEL_STD: [57.375, 57.120, 58.395]\n",
    "  RESNETS:\n",
    "    STRIDE_IN_1X1: False  # this is a C2 model\n",
    "    NUM_GROUPS: 32\n",
    "    WIDTH_PER_GROUP: 8\n",
    "    DEPTH: 101\n",
    "  ROI_HEADS:\n",
    "    NUM_CLASSES: 11\n",
    "SOLVER:\n",
    "  STEPS: (84000, 94500)\n",
    "  MAX_ITER: 15000\n",
    "  IMS_PER_BATCH: 8\n",
    "  BASE_LR: 0.06\n",
    "DATASETS:\n",
    "  TRAIN: (\"train\",)\n",
    "  TEST: (\"val\",)\n",
    "DATALOADER:\n",
    "  NUM_WORKERS: 8\n",
    "OUTPUT_DIR: \"/home/lpisaneschi/master_thesis/X101/models/finetuned_model_1044\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's finetune!\n",
    "\n",
    "Like a charm.\n",
    "\n",
    "```python\n",
    "def finetune_x101(example_img=None):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"/home/lpisaneschi/master_thesis/X101/configs/X101.yaml\")\n",
    "    num_gpu = 1\n",
    "    bs = (num_gpu * 2)\n",
    "    cfg.SOLVER.BASE_LR = 0.02 * bs / 16  # pick a good LR\n",
    "    model = build_model(cfg)\n",
    "    DetectionCheckpointer(model).load(\n",
    "        \"/home/lpisaneschi/master_thesis/X101/docbank_model.pth\")\n",
    "    register_coco_instances(\n",
    "        \"train\",\n",
    "        {},\n",
    "        f\"/home/lpisaneschi/master_thesis/X101/coco/1044_augmented_train2.json\",\n",
    "        \"/home/lpisaneschi/master_thesis/data/png/fully_annotated\")\n",
    "    register_coco_instances(\n",
    "        \"val\",\n",
    "        {},\n",
    "        f\"/home/lpisaneschi/master_thesis/X101/coco/1044_val.json\",\n",
    "        \"/home/lpisaneschi/master_thesis/data/png/fully_annotated\")\n",
    "    trainer = DefaultTrainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test it!\n",
    "\n",
    "```python\n",
    "def evaluate():\n",
    "    # register validation dataset\n",
    "    register_coco_instances(\n",
    "        \"val\",\n",
    "        {},\n",
    "        f\"/home/lpisaneschi/master_thesis/X101/coco/1044_val.json\",\n",
    "        \"/home/lpisaneschi/master_thesis/data/png/fully_annotated\")\n",
    "\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"/home/lpisaneschi/master_thesis/X101/configs/X101.yaml\")\n",
    "    # Use the final weights generated after successful training for inference\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0  # set the testing threshold for this model\n",
    "    # Pass the validation dataset\n",
    "    cfg.DATASETS.TEST = (\"val\",)\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    evaluator = COCOEvaluator(\"val\", cfg, False,\n",
    "                              output_dir=\"/home/lpisaneschi/master_thesis/X101/predictions/00predictions_no_augmentation_1044/\")\n",
    "    val_loader = build_detection_test_loader(cfg, \"val\")\n",
    "    # Use the created predicted model in the previous step\n",
    "    result = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- We demonstrated that Transformer power can be leveraged **outside NLP** tasks also\n",
    "\n",
    "- We have been able to **improve** detection performance using our **generative method**\n",
    "\n",
    "- We should refine **annotation pipeline** and exploit **NLP** to improve proposed method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align: center;\">THANKS FOR YOUR ATTENTION!</h1>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}